---
layout: post
title: "Natural Language Processing with Transformers"
date: 2025-11-30
tags: [NLP, Transformers]
categories: [Deep learning, Artificial Neural Networks, Natural Language Processing]
---
## Overview
This project involved implementing my understanding of Natural Language Processing. NLP focuses on enabling computers to understand, interpret and generate human language in ways that are both meaningful and useful. It has two subfields. Natural Language Understanding (NLU) which focuses on semantic analysis or determining the intended meaning of text and Natural Language Generation (NLG) which focuses on generating meaningful text.

## Objectives
- Apply a pre-trained BERT model for sentence encoding using Hugging Face Transformers.
- Extract token-level contextual embeddings.
- Demonstrate how BERT captures word meaning based on context (polysemy).
- Compute and interpret cosine similarity between token embeddings.
- Explain the importance of contextual embeddings in contrast to static word vectors.

## Download Report
You can download the full report here:

[ðŸ“„ Download PDF Report](/assets/files/Deborah kwamboka_assignment 11.pdf)
